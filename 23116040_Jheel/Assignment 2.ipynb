{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d538af74-b18e-40af-8b23-3e6910aa274e",
   "metadata": {},
   "source": [
    "Q1)\n",
    "1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837c9113-3cfe-43f2-a97f-8bfccb2284ba",
   "metadata": {},
   "source": [
    "$ \\text{Likelihood function p(x|$\\theta$) is}$ \n",
    "$$ p(x|\\theta) = \\frac{1}{\\sqrt{8\\pi}}exp\\left(-\\frac{(x-\\theta)^2}{8}\\right)$$\n",
    "\n",
    "$\\text{The prior distribution for $\\theta$ is}$\n",
    "$$ p(\\theta)= \\frac{1}{3\\sqrt{\\pi}}exp \\left(-\\frac{(\\theta-5)^2}{12}\\right)$$\n",
    "\n",
    "$\\text{Now, according to Bayes' theorem,}$\n",
    "$$ p(\\theta|x) \\propto p(x|\\theta)p(\\theta)$$\n",
    "$$ \\propto exp\\left(-\\frac{(x-\\theta)^2}{8}-\\frac{(\\theta-5)^2}{12}\\right) $$\n",
    "$$ \\propto exp\\left(-\\frac{(6-\\theta)^2}{8}-\\frac{(\\theta-5)^2}{12}\\right) $$\n",
    "$$\\propto exp \\left(-\\frac{\\left(\\theta - \\frac{74}{13}\\right)^2}{2\\times\\frac{36}{13}}\\right) $$\n",
    "$$ \\implies p(x|\\theta) \\sim \\mathcal{N}\\left(\\frac{74}{13}, \\frac{36}{13}\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74325496-a3eb-4a0d-b3cb-a992af6e0549",
   "metadata": {},
   "source": [
    "Q1) 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af732d6b-544d-474c-a250-ca530e814e4c",
   "metadata": {},
   "source": [
    "$$ a=\\frac {1}{9}, \\space \\space b=\\frac {n}{4}$$\n",
    "$$ \\mu_{post}= \\frac{\\frac{5}{9}+\\frac{n}{4}\\bar{x}}{\\frac{9n+4}{36}} = \\frac{20+9n\\bar{x}}{9n+4}$$\n",
    "$$ \\sigma^2_{post}= \\frac{36}{9n+4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdcd82f-9521-465d-a647-e00338a49d8b",
   "metadata": {},
   "source": [
    "Q1) 1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145cd7ed-c7a7-4802-ba5a-60bd38b761f2",
   "metadata": {},
   "source": [
    "$$ \\mu_{post}= \\frac{20+9n\\bar{x}}{9n+4}$$\n",
    "$$\\text{As n $\\to \\infty $, $\\mu_{post} \\to \\bar{x}$ }$$\n",
    "$$ \\sigma^2_{post}= \\frac{36}{9n+4}$$\n",
    "$$\\text{As n $\\to \\infty $, $\\sigma^2_{post}\\to 0$}$$\n",
    "$\\text{ This reflects how the posterior mean updates towards the observed data as more data points are accumulated.} $\n",
    "$\\text{And the reduction in variance indicates increased certainty about the true value of $\\theta$ as more data is received.}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1812ead0-e695-4f7d-8c54-4e5e278ce5b6",
   "metadata": {},
   "source": [
    "Q1) 1.4.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f216c85-7b90-4ef9-a4ce-29dc0750df74",
   "metadata": {},
   "source": [
    "$$ \\mu_{prior}= 100, \\sigma^2_{prior}= 152$$\n",
    "$\\text{Since, we have only one sample, $\\bar{x}= x = 80$.}\\space$\n",
    "$\\text{Therefore,}$\n",
    "$$ \\mu_{post}= \\frac{\\frac{100}{152} + \\frac{80}{102}}{\\frac{1}{152}+\\frac{1}{102}}= 88.03$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f913ddc1-22fd-4b00-a6b7-2c234e170da3",
   "metadata": {},
   "source": [
    "Q1) 1.4.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f928615d-ef13-4f34-8ebb-ddb64fc150cb",
   "metadata": {},
   "source": [
    "$\\text{Similarly, for $x=150$}$\n",
    "$$ \\mu_{post}= \\frac{\\frac{100}{152} + \\frac{150}{102}}{\\frac{1}{152}+\\frac{1}{102}}= 129.92$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d9dcd-2b96-4658-9de1-5b301c0ddfea",
   "metadata": {},
   "source": [
    "Q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d490f12f-611f-49bd-95e9-cd5b4ac2587c",
   "metadata": {},
   "source": [
    "$\\text{The likelihood function is:}$\n",
    "$$L(\\mu, \\sigma^2 \\mid \\{x_i\\}) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{(x_i - \\mu)^2}{2 \\sigma^2} \\right)$$\n",
    "\n",
    "$\\text{The log likelihood function is:}$\n",
    "$$ \\log L(\\mu, \\sigma^2 \\mid \\{x_i\\}) = -\\frac{n}{2} \\log(2 \\pi) - \\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2$$\n",
    "\n",
    "$\\text{To find the Maximum Likelihood Estimates, \\( \\hat{\\mu} \\) and \\( \\hat{\\sigma}^2 \\),  we maximize the log-likelihood function.}$\n",
    "$\\text{We get,}$\n",
    "$$ \\hat{\\mu}= \\frac{1}{n} \\sum_{i=1}^{n} x_i $$\n",
    "$$ \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\hat{\\mu})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a75de33e-bd8d-4c5e-babf-8c9cb2fc50cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True mean: 5.0, True standard deviation: 2.0\n",
      "MLE of mean: 4.909486604140559, MLE pf standard deviation: 1.9740663198660107\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "np.random.seed(0)  \n",
    "mean = 5.0\n",
    "std_dev = 2.0\n",
    "n = 1000\n",
    "data = np.random.normal(mean, std_dev, n)\n",
    "\n",
    "def neg_log_likelihood(params, data):\n",
    "    mu, sigma = params\n",
    "    n = len(data)\n",
    "    return (n / 2) * np.log(2 * np.pi * sigma**2) + np.sum((data - mu)**2) / (2 * sigma**2)\n",
    "\n",
    "initial_guess = [0, 1]\n",
    "\n",
    "result = minimize(neg_log_likelihood, initial_guess, args=(data,))\n",
    "    \n",
    "mu_mle, sigma_mle = result.x\n",
    "\n",
    "print(f\"True mean: {mu_true}, True standard deviation: {sigma_true}\")\n",
    "print(f\"MLE of mean: {mu_mle}, MLE pf standard deviation: {sigma_mle}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ec9009-ac1d-4aa9-8fab-922a2e1e9bb9",
   "metadata": {},
   "source": [
    "Q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aae7547d-b49d-4741-b9ba-d832763048e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Parameters:  [-0.5  1.   1.5  1. ]\n",
      "Estimated Parameters:  [-0.55124223  0.86040467  1.5612228   0.94275723]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def map(theta, x, mu, sigma, y):\n",
    "    z = np.dot(x, theta)  \n",
    "    log_likelihood = np.sum(y*np.log(sigmoid(z)) + (1-y)*np.log(1-sigmoid(z)))\n",
    "    log_prior = -0.5 * np.sum(((theta-mu)/sigma)**2) - (len(theta)/2) * np.log(2 * math.pi * sigma**2)\n",
    "    return log_likelihood + log_prior\n",
    "\n",
    "def neg_map(theta, x, mu, sigma, y):\n",
    "    return -map(theta, x, mu, sigma, y)\n",
    "\n",
    "np.random.seed(0)\n",
    "n_samples = 1000\n",
    "n_features=4\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "theta_true = np.array([-0.5, 1.0, 1.5, 1.0])\n",
    "\n",
    "y = (np.random.rand(n_samples) < sigmoid(np.dot(X, t_theta))).astype(int)\n",
    "\n",
    "mu_prior = np.zeros(X.shape[1])\n",
    "sigma_prior = 16\n",
    "starting_guess = np.zeros(X.shape[1])\n",
    "\n",
    "result = minimize(neg_map, starting_guess_1, args=(X, mu_prior, sigma_prior, y1))\n",
    "theta_estimated = result.x\n",
    "\n",
    "print(\"True Parameters: \", theta_true)\n",
    "print(\"Estimated Parameters: \", theta_estimated)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f445cb-50d5-49be-9adb-8cc00bb1eaf8",
   "metadata": {},
   "source": [
    "Q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5d4969-ff76-48e2-a02e-e9393ab7d413",
   "metadata": {},
   "source": [
    "4.1) \n",
    "Consider the case where there is only one point x1. The possible labellings are 0 and 1.\n",
    "The concept class can shatter this set because there exist hypotheses h_0 and h_1 that can assign labels 0 and 1 respectively.\n",
    "Consider two points x1 and x2. There are 2^2= 4 labellings possible,(0,0),(0,1),(1,0),(1,1).\n",
    "The concept class cannot shatter this set because  It cannot separate (0,0) and (1,1) simultaneously with any single constant function.\n",
    "Therefore, the VC dimension of the constant function concept class is 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de36838-5d5e-4865-b603-394930b20169",
   "metadata": {},
   "source": [
    "4.2) A linear function in d dimensions can be defined as h(x)=w'x+b, where, w is a d dimensional matrix.\n",
    "Consider the case where there is only one point x1 $\\in \\mathbb{R}^d$. The linear function can shatter this single point because w'x1+b>0 can be assigned label +1 and w'x1+b< 0 can be assigned label −1.\n",
    "Consider two points x1, x2 $\\in \\mathbb{R}^d$.\n",
    " There are 2^2= 4 labellings possible, (+1,+1),(+1,−1),(−1,+1),(−1,−1). The linear function concept class can shatter this set because it is possible to find a linear separator (hyperplane) that can separate all possible combinations of labels for two points in $\\mathbb{R}^d$. For d+1 points, the linear function concept class can shatter this set as well because there exists a hyperplane in d dimensions which can divide the space such that the d+1 datapoints can be classified.\n",
    "It fails to shatter sets of d+2 points because any hyperplane in d dimensions will not be able to classify all the datapoints uniquely.\n",
    "Therefore, the VC dimension of the concept class of linear functions in d dimensions is d+1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46519748-0ade-4ccd-a2ce-bd7f418e31a2",
   "metadata": {},
   "source": [
    "Q5) 5.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646ae49f-b9a9-4369-889d-72144d604039",
   "metadata": {},
   "source": [
    "$$ D_{KL}(P \\parallel Q) = \\int_{-\\infty}^{\\infty} p(x) \\log \\frac{p(x)}{q(x)} \\, dx$$\n",
    "$$ \\therefore D_{KL}(P \\parallel Q) = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi\\sigma_1^2}} \\exp \\left( -\\frac{(x - \\mu_1)^2}{2\\sigma_1^2} \\right) \\log \\left( \\frac{\\frac{1}{\\sqrt{2\\pi\\sigma_1^2}} \\exp \\left( -\\frac{(x - \\mu_1)^2}{2\\sigma_1^2} \\right)}{\\frac{1}{\\sqrt{2\\pi\\sigma_2^2}} \\exp \\left( -\\frac{(x - \\mu_2)^2}{2\\sigma_2^2} \\right)} \\right) dx$$\n",
    "\n",
    "$$ \\therefore D_{KL}(P \\parallel Q) = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi\\sigma_1^2}} \\exp \\left( -\\frac{(x - \\mu_1)^2}{2\\sigma_1^2} \\right) \\left[ \\log \\frac{\\sigma_2}{\\sigma_1} - \\frac{(x - \\mu_1)^2}{2\\sigma_1^2} + \\frac{(x - \\mu_2)^2}{2\\sigma_2^2} \\right] dx\n",
    "$$\n",
    "\n",
    "$ \\text{Upon solving,}$\n",
    "\n",
    "$$ D_{KL}(P \\parallel Q) = \\log \\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ece0ed-a10e-49da-bd07-b3fceaa99460",
   "metadata": {},
   "source": [
    "Q5) 5.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3421df2f-28e3-4585-8c6d-02fcc84aa311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>Q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.888609e-31</td>\n",
       "      <td>1.486720e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.888609e-29</td>\n",
       "      <td>2.438962e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.904861e-27</td>\n",
       "      <td>3.961301e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.275588e-25</td>\n",
       "      <td>6.369829e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.093301e-24</td>\n",
       "      <td>1.014086e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              P             Q\n",
       "0  7.888609e-31  1.486720e-07\n",
       "1  7.888609e-29  2.438962e-07\n",
       "2  3.904861e-27  3.961301e-07\n",
       "3  1.275588e-25  6.369829e-07\n",
       "4  3.093301e-24  1.014086e-06"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'C:\\Users\\91876\\OneDrive\\ドキュメント\\Desktop\\data_KL.csv',index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6cb88db6-d364-4ab2-a64c-59858fa03338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence from P to Q: 0.3182\n",
      "KL Divergence from Q to P: 0.8319\n"
     ]
    }
   ],
   "source": [
    "P = df['P'].values\n",
    "Q = df['Q'].values\n",
    "\n",
    "kl_div1 = np.sum(P * np.log(P / Q)) \n",
    "kl_div2 = np.sum(Q * np.log(Q / P)) \n",
    "\n",
    "print(f\"KL Divergence from P to Q: {kl_div1:.4f}\")\n",
    "print(f\"KL Divergence from Q to P: {kl_div2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85dc650-727b-44db-b25e-02d016aee644",
   "metadata": {},
   "source": [
    "Q5) 5.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a80d13-9a89-4cf2-9205-42a0467b4a97",
   "metadata": {},
   "source": [
    "KL divergence measures the divergence or disparity between two distributions, actual and expected. It is not mathematical distance, as it is not symmetric, that is, $D_{KL}(P\\parallel Q) \\neq  D_{KL}(Q\\parallel P) $.\n",
    "Moreover, it actually gives the infromation lost when one distribution is approximated as other."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
